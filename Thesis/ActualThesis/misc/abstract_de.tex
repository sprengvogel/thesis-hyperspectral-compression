\thispagestyle{empty}
\vspace*{1.4cm}

\begin{center}
    {\Large \textbf{Zusammenfassung}}
\end{center}

\vspace*{0.5cm}

\noindent 
Kompression von hyperspektralen Bildern mithilfe von neuralen Netzwerken ist ein wachsendes Forschungsgebiet innerhalb der Fernerkundungsforschung. In dieser Arbeit wird das Combined Model, eine Architektur zur Kompression von Bildern unter Nutzung von sowohl den örtlichen als auch den spektralen Abhängigkeiten in hyperspektralen Bildern, vorgeschlagen. Mithilfe des Combined Models können verschiedene spektrale und örtliche Autoencoder in Kombination miteinander eingesetzt werden. Zudem können örtliche Autoencoder, die ursprünglich für das Gebiet der Kompression von multispektralen oder RGB-Bildern entwickelt wurden, für die Kompression von hyperspektralen Bildern eingesetzt werden. Wir schlagen außerdem mehrere spezifische spektrale sowie örtliche Autoencoder-Modelle vor, die als Teil des Combined Model eingesetzt werden können, sowie eine Trainingsmethodologie, die für das Combined Model entwickelt wurde. Als Teil dieser Trainingsmethodologie wurde zudem eine neue Verlustfunktion entwickelt. Die Modelle wurden anschließend auf dem hyperspektralen Bilddatenset HySpecNet-11k evaluiert. Verschiedene Aspekte des Combined Models sowie die Trainingsmethodologie wurden analysiert und für verschiedene Bitraten verglichen. Die Ergebnisse zeigen, dass das Combined Model für niedrige und sehr niedrige Bitraten signifikant bessere Bildrekonstruktionen erzielt als sowohl bisherige hyperspektrale Kompressionsmethoden auf Basis von Machine Learning als auch die Kompressionsmethode JPEG 2000.