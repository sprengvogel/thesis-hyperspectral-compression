\chapter{Related Work\label{cha:chapter2}}
Learned hyperspectral image compression, as explained before, is a developing field of study. While there are some papers published on this topic, there are some problems making it difficult to assess and compare the results of these studies, as will be illuminated further in this chapter. Since learned RGB image compression is a closely related field and much more widely researched, the studies done regarding this topic are also analysed.
\section{Hyperspectral image compression \label{sec:ch2hyperspectral}}
Most learned hyperspectral image compression papers use a CNN-based model architecture to reduce the dimensions of the input image \citep{kuester_1d-convolutional_2021}\citep{kuester_transferability_2022}\citep{la_grassa_hyperspectral_2022}. Another model proposed by Guo et.al. \citep{guo_learned_2021} uses the hyperprior architecture, originally developed by Ballé et.al. \citep{balle_end--end_2017}, which also uses convolutional layers in an ANN but combines them with an arithmetic coder to improve compression rate.\\
Some other models that will be explored later are an SVM-based model by Aidini et.al. \citep{aidini_hyperspectral_2019}, a GAN model by Deng et.al. \citep{deng_learning-based_2020} and a model using a simple multi-layer perceptron by Kumar et.al. \citep{leal-taixe_onboard_2019}.
\subsection{CNN-based architectures}
The CNN-based architectures can be split into two categories, the first being the models using two-dimensional convolutional layers to learn the spatial dependencies of the hyperspectral images \citep{la_grassa_hyperspectral_2022}. The second category are models using one-dimensional convolutional layers to learn the spectral dependencies of the input data \citep{kuester_1d-convolutional_2021}\citep{kuester_transferability_2022}. Prior to the release of this thesis there are no purely CNN-based papers using both the spatial and the spectral dependencies of hyperspectral images for compression. The model proposed by Guo et.al. \citep{guo_learned_2021} does use both spatial and spectral dependencies, it does however use a hyperprior architecture and not a purely CNN-based model.\\
As said before, comparing the results from these papers directly is difficult. The reason for this is that the models use different data sets since there is currently no accepted standard data set for hyperspectral imaging. Furthermore, the models use different compression rates. Since a higher compression rate also leads to a higher compression error for the same model as described by rate-distortion theory \citep{berger_rate-distortion_2003}, this makes it impossible to directly compare the results from papers that use both a different data set and different compression rates.\\
Testing these models with a common data set revealed that for this dataset, the one-dimensional CNN model by Kuester et.al. \citep{kuester_1d-convolutional_2021} had the best performance of all the compared hyperspectral compression models, even including the non-CNN-based architectures, making it the current state of the art. This model is trained per pixel of the input image, making training slow but also resulting in good reconstruction accuracy.
\subsection{Other architectures}
An architecture different from the CNN-based models is proposed by Aidini et.al. \citep{aidini_hyperspectral_2019}. They use quantization to compress the original image, meaning that the resolution of the image is simply decreased. Then an algorithm tries to recover the original tensor values by trying to reconstruct low-rank tensors as a constrained optimization problem. Afterwards a spatial super-resolution algorithm using trained dictionary learning is used to increase the resolution of this image, after which a classifier is trained on these super-resolved images. While this architecture is interesting, it is not directly used in this thesis as its methodology would be very difficult to adapt to the neural network based methodolgies used in the thesis.\\
Similarly, other models based on non-ANN algorithms such as the work by Ülkü et.al. \citep{ulku_large-scale_2018} also using dictionary-learning and Zikiou et.al. \citep{zikiou_support_2020} using a support vector machine were not applicable for the methods used in the thesis.

Another category of models use ANNs to determine parameters for lossless compression algorithms. Shen et.al. \citep{shen_golomb-rice_2017} use a deep belief network to determine the optimal parameters for golomb-rice coding, a lossless coding algorithm that normally assumes a geometric underlying distribution. Using a neural network to determine the parameter removes that necessity.\\
This core strategy is also used by Guo et.al. \citep{guo_learned_2021}. They use a hyperprior architecture to compress hyperspectral images. Hyperprior models use an ANN-based model to transform the image data into a latent space that is commonly lower-dimensional than the input image. Then a second ANN is trained on the latent space to determine parameters for an arithmetic coder, a lossless coding algorithm. In both Guo et.al. and the original paper introducing the hyperprior architecture for RGB images, Ballé et.al. \citep{balle_end--end_2017}, both ANNs are CNNs and the latent space is indeed a lower-dimensional space.\\
Guo et.al. innovates on the original approach in the fact that they assume a student's T distribution instead of a gaussian distribution for the arithmetic coder and in the design of the first CNN. Their version includes both a spatial and a spectral part in the main CNN, making it the only model to learn both spatial and spectral information for hyperspectral image compression. However, a disadvantage of their model is that it is developed only for data sets with a low amount of channels compared to other hyperspectral datasets with the highest having 102 spectral channels. The approach is also not easily adaptable to datasets with a much higher number of channels.

\section{RGB image compression \label{sec:ch2rgb}}
\subsection{CNN-based architectures}
\subsection{Transformer-based architectures}