\chapter{Related Work\label{cha:chapter2}}
Learned hyperspectral image compression, as explained before, is a developing field of study. While there are some papers published on this topic, there are some problems making it difficult to assess and compare the results of these studies, as will be illuminated further in this chapter. An overview of hyperspectral image compression algorithms by Dua et.al. \citep{dua_comprehensive_2020} shows the discrepance between traditional transform-based and prediction-based compression techniques and techniques based on machine learning. The review contains 21 transform-based and 19 prediction based techniques but only five learning-based models. While the overview over the models based on machine learning is no longer complete since the review was done in 2020, it still shows that learning-based approaches for hyperspectral image compression have been a less researched field until recently.\\
Since learned RGB image compression is a closely related field and much more widely researched, the studies done regarding this topic are also analysed.
\section{Hyperspectral image compression \label{sec:ch2hyperspectral}}
Most learned hyperspectral image compression papers use a CNN-based model architecture to reduce the dimensions of the input image \citep{kuester_1d-convolutional_2021}\citep{kuester_transferability_2022}\citep{la_grassa_hyperspectral_2022}. Another model proposed by Guo et.al. \citep{guo_learned_2021} uses the hyperprior architecture, originally developed by Ballé et.al. \citep{balle_end--end_2017}, which also uses convolutional layers in an ANN but combines them with an arithmetic coder to improve compression rate.\\
Some other models that will be explored later are an SVM-based model by Aidini et.al. \citep{aidini_hyperspectral_2019}, a GAN model by Deng et.al. \citep{deng_learning-based_2020} and a model using a simple multi-layer perceptron by Kumar et.al. \citep{leal-taixe_onboard_2019}.
\subsection{CNN-based architectures}
The CNN-based architectures can be split into two categories, the first being the models using two-dimensional convolutional layers to learn the spatial dependencies of the hyperspectral images \citep{la_grassa_hyperspectral_2022}. The second category are models using one-dimensional convolutional layers to learn the spectral dependencies of the input data \citep{kuester_1d-convolutional_2021}\citep{kuester_transferability_2022}. Prior to the release of this thesis there are no purely CNN-based papers using both the spatial and the spectral dependencies of hyperspectral images for compression. The model proposed by Guo et.al. \citep{guo_learned_2021} does use both spatial and spectral dependencies, it does however use a hyperprior architecture and not a purely CNN-based model.\\
As said before, comparing the results from these papers directly is difficult. The reason for this is that the models use different data sets since there is currently no accepted standard data set for hyperspectral imaging. Furthermore, the models use different compression rates. Since a higher compression rate also leads to a higher compression error for the same model as described by rate-distortion theory \citep{berger_rate-distortion_2003}, this makes it impossible to directly compare the results from papers that use both a different data set and different compression rates.\\
Testing these models with a common data set revealed that for this dataset, the one-dimensional CNN model by Kuester et.al. \citep{kuester_1d-convolutional_2021} had the best performance of all the compared hyperspectral compression models, even including the non-CNN-based architectures, making it the current state of the art. This model is trained per pixel of the input image, making training slow but also resulting in good reconstruction accuracy.
\subsection{Other architectures \label{sec:ch2others}}
An architecture different from the CNN-based models is proposed by Aidini et.al. \citep{aidini_hyperspectral_2019}. They use quantization to compress the original image, meaning that the resolution of the image is simply decreased. Then an algorithm tries to recover the original tensor values by trying to reconstruct low-rank tensors as a constrained optimization problem. Afterwards a spatial super-resolution algorithm using trained dictionary learning is used to increase the resolution of this image, after which a classifier is trained on these super-resolved images. While this architecture is interesting, it is not directly used in this thesis as its methodology would be very difficult to adapt to the neural network based methodolgies used in the thesis.\\
Similarly, other models based on non-ANN algorithms such as the work by Ülkü et.al. \citep{ulku_large-scale_2018} also using dictionary-learning and Zikiou et.al. \citep{zikiou_support_2020} using a support vector machine were not applicable for the methods used in the thesis.

Another category of models use ANNs to determine parameters for lossless compression algorithms. Shen et.al. \citep{shen_golomb-rice_2017} use a deep belief network to determine the optimal parameters for golomb-rice coding, a lossless coding algorithm that normally assumes a geometric underlying distribution. Using a neural network to determine the parameter removes that necessity.\\
This core strategy is also used by Guo et.al. \citep{guo_learned_2021}. They use a hyperprior architecture to compress hyperspectral images. Hyperprior models use an ANN-based model to transform the image data into a latent space that is commonly lower-dimensional than the input image. Then a second ANN is trained on the latent space to determine parameters for an arithmetic coder, a lossless coding algorithm. In both Guo et.al. and the original paper introducing the hyperprior architecture for RGB images, Ballé et.al. \citep{balle_end--end_2017}, both ANNs are CNNs and the latent space is indeed a lower-dimensional space.\\
Guo et.al. innovates on the original approach in the fact that they assume a student's T distribution instead of a gaussian distribution for the arithmetic coder and in the design of the first CNN. Their version includes both a spatial and a spectral part in the main CNN, making it the only model to learn both spatial and spectral information for hyperspectral image compression. However, a disadvantage of their model is that it is developed only for data sets with a low amount of channels compared to other hyperspectral datasets with the highest having 102 spectral channels. The approach is also not easily adaptable to datasets with a much higher number of channels.

Another model using neural networks is proposed by Kumar et.al. \citep{leal-taixe_onboard_2019}. Instead of CNNs they use a simple multi-layer perceptron as the decoder for the reason that they use this model for real-time onboard image compression which requires a much more simple model for faster execution speed. Another uncommon trait of their architecture is that they do not use a symmetric model, meaning that the encoder and decoder are mirrors of one another. Instead, they only use an ANN for the decoder and use a low-complexity encoder based on matrix multiplication.

Hong et.al. \citep{hong_spectralformer_2022} propose an interesting architecture as well. They use a transformer that works on a spectral embedding by linearly projecting groups of neighbouring spectral bands to an embedding vector. This improves the capabilities of the network since neighbouring bands in hyperspectral images capture detailed changes in the absorption of the underlying material and therefore contain important information, especially for classification tasks.\\
In addition to this they implement Cross-layer Adaptive Fusion (CAF) to improve exchange of information in the transformer section of the model. This means that they use multiple transformer layers and, in addition to the direct connection between adjacent transformer layers, add connections that skip one layer and connect with the layer after using a special CAF module.\\
The transformers can be applied either per-pixel or for small patches. However, their work is not used in the context of image compression but rather image classification and therefore only contains an encoder combined with an MLP head to classify hyperspectral image pixels or patches based on categories such as "Corn", "Grass Pasture" or "Wheat". This means that an application of this architecture to the task of image compression would require substantial additions to the model. 
\section{RGB image compression \label{sec:ch2rgb}}
While the study of hyperspectral image compression strongly increased in recent years, the same does not hold for image compression of traditional RGB images. There are many traditional compression algorithms, some of which are installed on every modern operating system and browser. Examples of these are PNG, a lossless image compression format as well as JPEG, a collection of compression algorithms, the most common of which performs lossy compression of images. \\
However, while these algorithms are very popular, learning-based image compression has outperformed methods such as JPEG in both compression ratio and distortion. Furthermore, many of the ideas in learned hyperspectral image compression originate from RGB image compression studies and many of the ideas in RGB image compression have not yet been adapted to the hyperspectral realm. For these reasons RGB image compression papers are directly relevant even for a thesis that only concerns itself with the hyperspectral domain.
\subsection{The hyperprior architecture}
One of the most important recent works in RGB image compression was released by Ballé et.al. \citep{balle_variational_2018} for the International Conference on Learning Representations (ICLR) 2018. This paper builds on a previous work using a CNN to reduce the dimensionality of the input image, followed by a quantization of the resulting latent and the usage of an arithmetic autoencoder to losslessly compress the quantized latent \citep{balle_end--end_2017}. The output of the arithmetic autoencoder is then decoded by a CNN that is symmetrical to the encoder CNN, similar to the models for hyperspectral image compression that were already discussed. This model already outperformed JPEG and the improved JPEG 2000 on the tested data.

The performance of the arithmetic autoencoder depends on the accuracy of the estimated probability distribution it uses for the data that it compresses. This area is where improvements were found. Ballé et.al. used a smaller, separate CNN that learns to extract parameters for a good probability distribution estimate from the latent resulting from the main CNN. This network also uses an autoencoder structure, meaning that the estimate can also be transmitted in compressed form as side channel information using only a small amount of space. Training a network using gradient descent for this purpose would not ordinarily be possible since the quantized latents have discrete values resulting in zero gradients everywhere. For this reason the quantization is substituted during training by addition of a small amount of uniform noise to dediscretize the latents.\\
This hyperprior architecture is the basis or a component of a large portion of modern learned RGB image compression models and also one hyperspectral image compression model that was discussed in Chapter \ref{sec:ch2hyperspectral}.

One such example is a paper by Hu et.al. \citep{hu_coarse--fine_2020} where the model is generalised to include not only two but an adaptable number of CNNs, where each CNN learns the probability distribution of the CNN before it. This leads to slight improvements in the bitrate of the model while not changing the distortion. The distortion remains unchanged since the only loss occurs within the first CNN which compresses the input image . The other CNNs are only used to improve the performance of the lossless arithmetic coders.

The architecture was also further improved in two ways by Minnen et.al. \citep{minnen_joint_2018}. The first improvement is a generalisation in the structure of the probalitiy distribution from the original scale mixture of gaussians \citep{wainwright_scale_1999} to a gaussian mixture model. This means that the second CNN generating the probability distribution parameters has to predict both a scale and a mean instead of only a scale as before. This allows for a better modeling of the true underlying distribution and the paper shows that the increase in necessary side channel information is lesser than the improvement created by the improvement in probability distribution prediction.\\
The second new idea is the addition of an autoregressive model over the latents of the first, main CNN. This also improves compression performance as the more structure from the latents can be exploited, it does however also increase the computational costs of the network as autoregressive models cannot be trained in parallel. 

Another application of the hyperprior architecture is found in Cheng et.al. \citep{cheng_learned_2020}.

\subsection{Transformer-based architectures}