\chapter{Theoretical Foundations\label{cha:chapter3}}

This chapter describes the implementation of component X.

\section{Convolutional Neural Networks}

\section{Transformer Models}

\section{Arithmetic Coding}
Arithmetic coding is an algorithm used for entropy coding, that is a technique for encoding and decoding a sequence of symbols in an efficient way \citep{witten_arithmetic_1987}. As an example, a string of characters can be considered. A standard non-optimized way of storing a string is by using the \ac{ascii} encoding which used a fixed seven bytes per character of the string. Entropy coding algorithms improve on this by allocating fewer bits to more common characters and therefore more bits to less common characters \citep{witten_arithmetic_1987}. In total, this leads to a lower number of required bits for the total message, given an accurate probability model for the frequencies of the characters. In fact, if the provided probability model is perfectly accurate, arithmetic coding yields an encoding close to an optimal encoding, that is an encoding that uses a number of bits equal to the entropy of the encoded input \citep{witten_arithmetic_1987}. This is the fewest number of bits possible for a lossless encoding of an arbitrary message, which was proven with Shannon's source coding theorem \citep{shannon_mathematical_1948}\citep{mackay_information_2003}.

Arithmetic coding, using the theoretical version of the algorithm, works by encoding the entire input message into one arbitrary-precision rational number $q$ with $0 \leq q < 1$ \citep{said_introduction_2023}. In practice the algorithm is slightly modified to account for the lack of infinite precision in computers, as will be detailed later in this section. The encoding is performed using an iterative process. In order to understand this process, consider for an example an alphabet consisting of three symbols, 'A', 'B' and 'C' with 'A' having the probability $0.5$, 'B' having the probability $0.3$ and therefore 'C' having the probability $0.2$. Encoding an message is performed symbol by symbol. To encode the first symbol, the interval $[0,1)$ (meaning numbers $a$ with $0 \leq a < 1$) is split into three parts according to these probabilities. If the first symbol is 'A', the final encoding number $q$ will be in the interval $[0,0.5)$. Likewise, if the first symbol is 'B' or 'C', $q$ will be in the intervals $[0.5,0.8)$ or $[0.8,1)$ respectively. The second symbol is then encoded by splitting the interval determined from the first symbol again, using the same probabilities. A sequence of two 'A' symbols would therefore result in $q$ being in the interval $[0,0.25)$. This procedure is iterated until each symbol of the input message is encoded, resulting in a single number $q$ representing the entire input.

\section{Hyperprior Architecture}