\chapter{Methodology\label{cha:chapter4}}
As discussed in \autoref{cha:chapter2}, there is a much research on learning-based \ac{rgb} image compression. Therefore, being able to use and further research methods used in that field for hyperspectral image compression would be very useful. However, direct application of most learning-based \ac{rgb} image compression methods on hyperspectral data is not possible. This is mainly for two reasons. Firstly, learning-based \ac{rgb} image compression methods ignore dependencies between the three channels in the \ac{rgb} images and focus solely on spatial dependencies. This is useful for \ac{rgb} images because the spectral dependencies within the three channels are low, since the corresponding wavelengths are far apart. Additionally, having only three channels means that there is not much spectral information per pixel to be compressed, meaning that even if spectral dependencies existed, the compression gains that could be extracted would be small. This is different in hyperspectral image compression where there are large amounts of spectral dependencies in most datasets and the potential compression gains are much larger because of the high number of spectral channels. It is therefore necessary to modify \ac{rgb} models to incorporate spectral dependencies in order to adapt them to the task of hyperspectral image compression. 

The second reason for the difficulty in applying \ac{rgb} image compression methods to hyperspectral image is the strong increase in dimensionality. Compared to an \ac{rgb} image, a hyperspectral image of the same size with 200 channels has $200/3 \approx 67$ times more dimensions. For this reason \ac{rgb} models are often too small to learn high-dimensional hyperspectral image data. For example, many \ac{cnn} models use an input layer transforming the input from the dimensions H x W x C, where H and W are the height and width and C is the number of channels to H' x W' X N, where H and W are often either the original height and width or half the original height and width and N is the number of filters used in the input layer. In \ac{rgb} models, 128 and 192 are common values for N. For \ac{rgb} images, this means that the input layer increases the dimensionality drastically in order to learn multiple features of the image. However, in hyperspectral images, the same layer would result in a decrease of dimensionality leading to loss of information and therefore a higher distortion for the complete compression model. A simple idea in order to address this issue is to increase the number of filters in each layer according to the increase in dimensionality between \ac{rgb} and hyperspectral images. The input layer would then for example have $192 * 67 = 12864$ filters. However, this idea is not feasible for this thesis since training models modified in this way is not possible on the available hardware. Additionally, even if the models were trainable, it is unclear if the convolutional layer would be able to learn 12864 distinct features for hyperspectral images.

Therefore, a different model architecture is needed. This thesis presents a novel architecture that allows the use of spatial compression methods designed for an almost arbitrary number of channels in hyperspectral image compression while also incorporating spectral dependencies. This not only allows the use of \ac{rgb} image compression methods but also multispectral compression methods. Furthermore, it is also possible to use non-learning-based methods using a desired number of channels, like \ac{jpeg} 2000, on hyperspectral data while integrating spectral dependencies of hyperspectral images. This architecture is called "Combined Model".
\section{The Combined Model}
The main idea of the combined model is to use a spectral encoder network that preserves spatial dependencies in order to reduce the number of spectral channels in the input. Then, a spatial compression method is applied on the output of that spectral encoder in order to remove the spatial dependencies. The resulting bottleneck is then decoded using the corresponding spatial decoder. Finally, a spectral decoder network symmetrical to the spectral encoder restores the original image. During training, the spectral decoder learns to adapt to the distortion introduced by the spatial autoencoder. If the spatial autoencoder is learning-based, the spatial autoencoder similarly learns to produce output in such a way that the spectral autoencoder can better decode it. This mechanism is studied in detail in \autoref{cha:chapter5}. Adaptation in either the spatial autoencoder or the spectral autoencoder are necessary, especially if the latent space of the spectral autoencoder is not continuous. In that case, a small change introduced by the spatial autoencoder could result in a large change in the output of the spectral decoder. In practice the adaptation process is successful in avoiding chaotic outputs of the spectral decoder, which is also shown in \autoref{cha:chapter5}.

The aforementioned approach has multiple advantages. It is highly flexible with regard to the spatial autoencoder. Many different learning-based approaches can be used and the number of input channels to the spatial autoencoder can be modified so it best fits the spatial autoencoder. This can be achieved by modifying the compression ratio of the spectral encoder/decoder pair, which is possible for all spectral autoencoder models studied in this thesis. Another advantage is that the distinct separation between the spatial and the spectral encoding allows for informative studies to be done both regarding explainability and regarding the amount of spatial and spectral dependencies in the data. The combined model further allows for traditional methods as the spatial autoencoder, using the ability of the spectral decoder to adapt to the distortion introduced by the spatial autoencoder. The model also allows for different spectral encoder and decoder architectures. There are two necessary conditions required for a spectral encoder/decoder pair to be possible for use in the combined model. Firstly, the output of the encoder must be an image-like shape in order for spatial image compression models to be used for incorporating the spatial dependencies. Secondly, the spectral encoder needs to preserve spatial dependencies, ideally resulting in an output with similar spatial dependencies to the actual input image. Because the spatial autoencoder models were designed for image compression, having spatial dependencies close to the dependencies found in images means that the task the spatial autoencoder is used for is close to the one it was designed for.

However, a difference between the task of image compression and the task the spatial autoencoder performs, that being compression of the latent of the spectral autoencoder, is still present. This is one of the disadvantages of the combined model approach and therefore requires the spatial autoencoder to be resilient to this change of task. However, for the spectral autoencoder models studied in this thesis, the spatial dependencies of the latents very closely resemble the spatial dependencies of the original input, mitigating this disadvantage. For further detail on this see \autoref{cha:chapter5}.

For training the combined model, an additional idea is useful. If the model is trained in a standard way by training both the spatial autoencoder and the spectral encoder/decoder pair from a newly initialized state at the same time, the results can be unstable. Depending on the hyperparameters, it may not be possible to tŕain the combined model in this way. This follows from the fact that inputs to the spatial autoencoder depend on the outputs of the spectral encoder. During the start of training, the spectral encoder often outputs similar or equal values for all inputs. The spatial autoencoder may then specifically learn to compress latents with these specific values, which is a task very different from compressing spatial dependencies. In this case, the dependencies between the model parts can inhibit training and lead to the model being stuck in the local minimum of outputting the same output for any input. We solve this problem by pretraining the spectral encoder/decoder pair as an autoencoder without any spatial component. Then the combined model is trained using the pretrained spectral encoder and decoder. In this way, the spatial autoencoder receives useful input data from the beginning of training. 

An interesting modification option for the model follows from pretraining the spectral encoder/decoder pair. After pretraining the spectral component, the combined model can be trained while freezing the spectral component, training only the spatial autoencoder. If trained in this way, the combined model can be seen as a spatial image compression model using an involved preprocessing and postprocessing step. This option is analysed and compared with the normal method of training the combined model in \autoref{cha:chapter5}.
\section{Spectral Autoencoder Methods}
-Introduction sentence
\subsection{Pixel-Wise Convolutional Neural Network\label{sec:conv1d}}
-Describe model
-Describe changes from paper (variable number of layers for diff. compression ratios. Number of filters increased, possible advantage that it is handy with adaptable number of layers.)
-Model has the best reconstruction performance compared to any other model on our data (ref experiments)
-Slow training (why?)

The first of the spectral autoencoder methods is based on the model proposed in Kuester et.al. \citep{kuester_1d-convolutional_2021,kuester_transferability_2022}. In their proposed method a hyperspectral input is split up into its constituting pixels. The spectrum for each pixel then forms one datapoint on which the model is trained. The encoder of the model being trained is a \ac{oned} \ac{cnn} consisting of two sets of a \ac{oned} convolutional layer with a large kernel size of 11 and a max pooling layer followed by two additional \ac{oned} convolutional layers with a kernel size of 9 and 7. The number of filters is also reduced by a factor of two in each convolutional layer and set to one in the last layer, resulting in a bottleneck with one fourth of the original spectral channels. The decoder is built analogously, only replacing the max pooling layers by upsampling layers. After each convolutional layer a \ac{lrelu} is used as the activation function. Additionally, the number of channels are padded with channels containing zero values in order for them to be divisble by two. Since the number of channels is reduced by a factor of four during encoding, this can result in the bottleneck not having exactly one fourth of the channels of the input, if the number of channels in the padded input is not divisible by four. The decoder then results in one channel more than the original input. In this case, the last channel of the output is removed. A whole image can be compressed by encoding and decoding each pixel of the image separately, then reassembling the output image from the decoded pixels.

This model is highly compatible with the combined model approach. Because the encoding of the spectral channels is performed per pixel, spatial dependencies are guaranteed to remain during encoding. The latent image is produced similarly to the output of the decoder by reassembling the latents for each pixel into a latent image, which can then be further encoded using a spatial autoencoder. In order to optimize the model for use in the combined model architecture and for the used dataset HySpecNet-11k \citep{fuchs_hyspecnet-11k_2023}, we adapted the model in various ways.

In the model as proposed by Kuester et.al. \citep{kuester_1d-convolutional_2021,kuester_transferability_2022}, the number of layers and therefore the compression ratio is fixed to four. However, different datasets can have different amounts of spatial and spectral dependencies. In the context of the combined model, this can result in the optimal model requiring a different spectral compression factor than 4. To allow for different compression ratio, the number of sets of a convolutional and a max pooling layer was made variable. In addition to this, the number of filters in each convolutional layer was adapted to the number of channels in the data. In the original paper, the first layer has a number of filters equal to half the number of padded channels of the data. We adapted the number of filters to the higher number of channels in the HySpecNet-11k dataset by keeping this ratio equal. This has the advantage that the number of filters is always appropriately divisible regardless of the number of layers ensured by the padding on the spectral channels of the input. We also changed the padding strategy. Instead of padding to a number of spectral channels divisible by half the compression ratio and then conditionally removing some number of channels at the output, we directly padded to a number of spectral channels divisible by the compression ratio. This removes the need for removing channels at the output, simplifying the model without loss of performance.

This spectral compression model is not only interesting for its applicability to the combined model. It is also the hyperspectral compression model with the highest reconstruction accuracy for multiple bitrates among many compared models. For more on this see \autoref{cha:chapter5}. However, there are also some disadvantages of the model architecture. The reduction of spectral channels using max pooling layers results in divisions by two in the number of channels. This means that the compression ratio can only be a power of two. Additionally, the higher the chosen compression ratio is, the higher the number of needed padding channels is. As an example, for the HySpecNet-11k dataset with 202 channels two padding channels are needed for the compression ratio 4, for a compression ratio of 32 14 padding channels would be needed. Finally, the training process of this model is much slower than for which take complete images as input. Since each pixel is compressed separately, the number of optimization steps per epoch is much higher. This is also further discussed in \autoref{cha:chapter5}.
\subsection{Two-dimensional Spectral CNN\label{sec:fastconv1d}}
To address the issues of the \ac{oned} spectral \ac{cnn}, we developed another model. This model uses \ac{twod} convolutional layers with a kernel size of one to compress a whole image at once. For convolutional layers with a kernel size of one the convolution operation collapses down to a scalar multiplication. Additionally, the output has the same height and width as the input, as each pixel in the output is calculated only from the channels of a corresponding input pixel. The output for a specific pixel and filter is therefore equivalent to a weighted sum over all channels in the input for this pixel. Importantly however, the weight used for a specific input channel is the same for each pixel in the image. A two-dimensional convolutional layer with a kernel size of one can therefore only encode spectral information when applied to a hyperspectral image, making it usable as part of the combined model. Such layers are commonly used for decreasing the number of filters between other convolutional layers TODO REF. Because of this, they are also referred to as channel or feature map pooling layers. They are then able to learn non-trivial summarizations of the preceding layer because of the nonlinearity introduced by the used activation function, in our case \ac{lrelu}. However, they can also instead be used to the opposite effect of increasing the number of filters while retaining the semantic information within these filters. An example of both of these uses is present in the "Inception" architecture proposed in Szegedy et.al. \citep{szegedy_going_2014}.

We show that it is also possible to use such layers to learn spectral dependencies in hyperspectral image data. The model is comprised of two-dimensional convolutional layers starting with a number of filters slighly higher than the number of channels in the hyperspectral dataset the model is trained on, 256. In each layer, the number of filters is doubled in order to increase the amount of information available. Finally, a last convolutional layer is used to reduce the number of channels to arrive at the desired bottleneck size. All convolutional layers use a kernel size of one and the \ac{lrelu} activation function.

This architecture has multiple advantages compared with the one-dimensional spectral \ac{cnn}. Instead of being applied per pixel, it can be applied to the entire image at the same time. It also requires less parameters, as each layer has a number of parameters equal to $N_p = C * F$, where $C$ is the number of channels of the input to the layer and $F$ is the number of filters in the layer. Both of these factors result in a much shorter training time as seen in \autoref{cha:chapter5}. The model is also more flexible with regard to the compression ratio. In contrast to the one-dimensional spectral encoder, where the compression ratios are necessarily given by $R \approx 2^N$, where $N$ is the number of convolutional layers, the two-dimensional spectral encoder allows setting of the number of output channels to an arbitrary number. This also removes the need for padding, which is an dvantage especially for high compression ratios which would otherwise require much padding.

Two modifications were also tested for this model. First, a Layer Normalization was performed after each convolutional layer in order to improve generalization performance and further reduce training time.  We also modified the model by changing the kernel sizes of the convolutional layers. With higher kernel sizes, the model becomes able to incorporate some amount of local spatial dependencies in addition to the spectral dependencies. We used this to study how sensitive the combined model is to spatial dependencies being partially encoded in the spectral encoder/decoder.
\subsection{Variational autoencoder \label{sec:vae}}
-Describe model and background in detail
-Idea of VAE for continuous latent space
-However, not useful
-Hyperprior sadly not possible here because bottleneck is not compressible further.

\section{Spatial Autoencoder Methods\label{sec:spatialae}}
-Introduction
\subsection{CNN-based Spatial Autoencoder}
-Describe model

\subsection{Hyperprior-based Spatial Autoencoder}
-Describe model, both original ballé and with CNN-base

\subsection{Attention-based Model Using Hyperprior Architecture}
-Describe model and how to change for multiple spatial compression ratios while adapting hyperprior
-Hyperprior bottleneck has to be kept small
