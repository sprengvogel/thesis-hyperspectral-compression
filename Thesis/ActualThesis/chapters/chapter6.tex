\chapter{Discussion and Conclusion\label{cha:chapter6}}
This thesis investigates the capabilities of utilising a spatio-spectral approach to learning-based hyperspectral image compression. To this end, a novel architecture is proposed that combines a spectral and a spatial autoencoder by applying the spatial autoencoder to the compression bottleneck of the spectral autoencoder. We call this architecture Combined Model. Multiple different approaches are used for both the spectral and spatial parts of the Combined Model. Additionally, a method of training optimised for the Combined Model architecture is proposed. This method employs pretraining of the spectral autoencoder in conjunction with freezing parts of the spectral autoencoder during actual training as well as a novel loss function called \ac{dualmse} loss to improve the capabilities of the Combined Model.

For spectral encoding we propose two main model types. The first method is a \ac{oned} \ac{cnn} model that performs compression in the spectral dimension separately per pixel of an input image. We modified the model to allow for variable compression ratios. Furthermore, we adapted the model to the dataset used in this thesis by changing the number of filters in each layer and changed the padding strategy to be more compatible with a variable compression ratio. The second method is a \ac{twod} \ac{cnn} model that employs a kernel size of 1 in the convolutional layers to preserve spatial dependencies in the input while allowing for faster training and full control over the size of the latent dimension. Thirdly, a spectral variational autoencoder is also proposed. However, because experiments show that this model is not viable for spectral autoencoding. Therefore the focus of this thesis is on the other two spectral compression models.

For spatial encoding three main architectures are proposed. The first is a \ac{twod} \ac{cnn}-based model that reduces the spatial dimensionality of the input by a fixed compression ratio. This model is designed to have low complexity to be a baseline for the other spatial compression architectures as well as allow for testing of the Combined Model architecture and the proposed training method. The second spatial encoding architecture is a gaussian scale hyperprior that serves as a second baseline to the third spatial encoding architecture, which also employs a hyperprior.
The third spatial encoding architecture is a state-of-the-art hyperprior-based model using a gaussian mixture model hyperprior, a context model and self-attention modules. Additionally, the model is adapted to the Combined Model architecture by reducing the amount of spatial dependencies exploited by the main autoencoder and consequently increasing the amount of spatial dependencies exploited in the hyperprior autoencoder.

The performance of the different model architectures, specific modifications to the models and the training method designed for the Combined Model was tested on the HySpecNet-11k dataset, which provides large-scale hyperspectral image data of the surface of the earth with high spectral resolution. The models were evaluated on the distortion metrics \ac{psnr} and spectral angle as well as the achieved compression bitrate.

We conducted experiments to assess the effect of the Hughes Phenomenon on the Combined Model which showed that the effect on both the Combined Model and the \ac{twod} \ac{cnn}-based spectral autoencoder are in line with typical expectations for machine learning tasks. With regard to the specific training method, we showed that pretraining the spectral autoencoder and freezing the spectral encoder during training leads to significant improvements in \ac{psnr} and spectral angle for a fixed bitrate. We further showed that the effect of freezing the spectral encoder during training is increased when the latent dimension of the spectral encoder is small. In addition to this, while the use of the novel \ac{dualmse} loss does not significantly decrease distortion, it is successful in reducing the distortion between the latent of the spectral encoder and its reconstruction obtained after applying the spatial autoencoder to that latent by multiple orders of magnitude. The \ac{dualmse} loss is therefore potentially useful for an application of the Combined Model architecture that relies on a latent reconstruction with low distortion. To achieve this result, we exploited that the Combined Model architecture allows for a high degree of explainability by studying the spectral latents. Experiments also showed that the Combined Model achieves significantly lower distortion than all compared benchmarks on bitrates at or below 0.5 bpppc. We thereby showed that exploiting both spatial and spectral dependencies of hyperspectral images is essential to improve the performance of learning-based hyperspectral compression methods on the HySpecNet-11k dataset. Further, the Combined Model significantly improves the use of hyperprior-based models on hyperspectral data, achieving lower distortion than hyperprior-based models without the Combined Model approach and significantly lower bitrates at comparative distortion levels than all models not using a hyperprior. The Combined Model with the hyperprior-based spatial encoder using self-attention modules is to our knowledge the only learning-based hyperspectral image compression model able to achieve very low bitrates ($<0.02$ \ac{bpppc}) with low distortion. We therefore propose new state of the art learning-based hyperspectral compression models for low and for very low bitrates.

We see multiple possible areas of future research that could be explored to improve on the results from this thesis. The latents of the \ac{cnn}-based spatial autoencoder show that there are still spatial dependencies present that could potentially be exploited by a different spatial autoencoder architecture. Additionally, while different spectral compression ratios were extensively tested, more research could be performed into different spatial compression ratios. While the hyperprior-based spatial autoencoder achieves this through the use of the arithmetic coder, it requires a quantization step in order to apply the arithmetic coder, which leads to some amount of irreversible loss during the encoding process. We showed in the experiments removing the quantization step significantly reduce distortion. However, since the arithmetic autoencoder is then unable to be used, this is not directly for compression. A novel way to perform the quantization with less loss of information or an approach to possibly not require quantization at all could significantly improve the hyperprior-based models. Improvements in this area could also potentially translate to the application of hyperprior-based \ac{rgb} image compression. Furthermore, experiments were only performed on the HySpecNet-11k dataset. Research into the application of the models proposed in this thesis on different domains could provide more insight into the generalisability of the models proposed in this thesis. 