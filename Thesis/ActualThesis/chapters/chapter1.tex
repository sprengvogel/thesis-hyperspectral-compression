\chapter{Introduction\label{cha:chapter1}}
Hyperspectral imaging is a quickly growing field. It is the technique of capturing images with a specialized camera in order to obtain a spectrum of many wavelengths of light for each pixel of a taken image. There are three categories of hyperspectral cameras that are used to capture such images. The first, push broom scanners, use a linear arrangement of spectroscopic sensors, that are sensors able to capture a spectrum of many wavelengths of light at once. The sensor arrangement is then moved over the subject of the image, for example by being attached to a satellite orbiting earth. The second category, whisk broom scanners, functions similarly to push broom scanners with the difference being that the stationary sensor arrangement is replaced by a moving mirror reflecting light into a single detector that collects the spectral information in a step-by-step manner. Lastly, snapshot hyperspectral imaging works by using a sensor array to capture a complete hyperspectral image with a single activation of the sensors. 

Regardless of capture method, there are many applications for the use of hyperspectral images since a multitude of information can be extracted from their combination of spatial and spectral data. One such application is pixel-wise classification of materials, that being the process of determining the material of specific pixels in a hyperspectral image. This is possible because of the high amount of information present per pixel resulting from the large amount of spectral information. An instance of this approach can be seen in Zea et al. \citep{zea_leveraging_2022}. They use classification of hyperspectral images in order to detect the presence of the toxic metal cadmium in soil, lessening the need for chemical methods that require the plant to be harvested to test for cadmium stress. Using hyperspectral imaging the detection of cadmium can be performed on living plants. Another example of pixel-wise classification materials is explored in Henriksen et.al. \citep{henriksen_plastic_2022}, where it is used to separate plastic waste by twelve kinds of plastics with a higher accuracy than previously used methods such as near-infrared technology.

Another field where the use of hyperspectral imaging is rising in importance is in geology and environmental sciences. An example of this is the \ac{enmap} mission which launched a satellite into earths orbit in 2022. It takes images of the surface of the earth with a comparatively high \ac{gsd} of 30 square meters per pixel, allowing for regional geographic analysis \citep{guanter_enmap_2015}. In the spectral domain, the satellite yields a high resolution. By combining a sensor in the \ac{vnir} spectrum and a sensor in the \ac{swir} spectrum it can capture light with wavelengths between 420 and 2450 nm. Wavelengths between 420 and 1000 nm are captured by the \ac{vnir} sensor with a resolution of 6.5 nm per spectral band, while wavelengths between 900 and 2450 nm are captured by the \ac{swir} sensor with a resolution of 10 nm per band.
The \ac{enmap} mission has already lead to many interesting studies including of glacier ice surface properties of the ice sheet in South-West Greenland, moisture content of soil below grassland and identification of specific crop traits such as the chlorophyll content or the leaf water content \citep{bohn_glacier_2022,pascual-venteo_prototyping_2022,dopper_estimating_2022}.
A dataset created from the images produced by the \ac{enmap} mission also serves as the main dataset studied in this thesis.

Another ongoing hyperspectral imaging mission is the \ac{prisma} mission led by the \ac{asi} that was launched in 2019 \citep{loizzo_prisma_2019}. Similar to the \ac{enmap} mission, the \ac{prisma} mission also consists of a satellite that observes earth using both a sensor for the \ac{vnir} spectrum and for the \ac{swir} spectrum. It also has a \ac{gsd} of 30 square meters per pixel and a slightly lower spectral resolution given as less than 12 nm per band \citep{guarini_overview_2017,guarini_prisma_2018}. In contrast to the \ac{enmap} mission the satellite also carries a panchromatic camera, meaning that the camera only has a single spectral band. This camera however has a higher \ac{gsd} of 5 square meters per pixel. The data from the \ac{prisma} mission has been used to increase spatial resolution in hyperspectral images. This was done by combining the original hyperspectral images with multispectral images with a higher spatial resolution in a process called hyperspectral-multispectral fusion \citep{acito_prisma_2022}. The term 'multispectral image' refers to images with more spectral bands than traditional \ac{rgb} images, but less bands than hyperspectral images. Here the multispectral images contain 10 bands and are obtained from the \ac{s2} project, a mission by the \ac{esa} that continually performs multispectral imaging with fine spatial resolution \citep{drusch_sentinel-2_2012}. Another use of the \ac{prisma} data has been in disaster monitoring where it has been used to segment wildfires into areas of active fire, smoke, burned ground, bare soil and remaining vegetation \citep{spiller_transfer_2022}. Another related system developed using \ac{prisma} data performs wildfire detection in real-time from satellites  \citep{spiller_wildfire_2022}.

The increasing usage of hyperspectral imaging makes it essential to address the disadvantages of the technology. One major disadvantage of this type of imaging is the required amount of disk space for the resulting images. Because there are many more spectral bands compared to the three bands of \ac{rgb} in traditional photography the resulting file size rises accordingly. Furthermore, hyperspectral images often use a high precision for the specific brightness values captured for each point in the image. The \ac{enmap} satellite images are composed of 32 bit values, while in \ac{rgb} imaging 8 bits per pixel per band is most common \citep{guanter_enmap_2015}. A hyperspectral image might for example have 300 bands. In combination with 32 bit values per pixel per band the resulting file size would be 400 times larger than an \ac{rgb} image with the same pixel density. In addition to this it is important to notice that even \ac{rgb} images need to be compressed for many use cases in order to be efficiently stored and transmitted, which is why there are many compression standards for \ac{rgb} images in wide usage.
Furthermore, compression of hyperspectral images is a complex problem. This arises from the combination of spatial image compression complexities and the added challenge of encoding information in the spectral dimension which \ac{rgb} compression algorithms do not consider. The latter is one of the reasons why many \ac{rgb} compression algorithms do not perform well on hyperspectral data as will be shown in this thesis. In addition to this there are more technical reasons for the difficulty of adapting algorithms for \ac{rgb} compression that will also be explored in this thesis. For the above mentioned reasons research into compression algorithms for hyperspectral images is of vital importance.

Algorithms for image compression can be categorized into multiple broad groups. They can firstly be grouped by whether they compress the image losslessly or with loss. Lossless algorithms restore the compressed image exactly, whereas lossy reconstructions cause distortion. The disadvantage of lossless compression is however that there are mathematical limits to the compression rate given by the entropy in the data that is to be compressed. This limit is given by Shannon's source coding theorem \citep{shannon_mathematical_1948}, which has been stated as follows \citep{mackay_information_2003}:

\begin{quotation}
$N$ independent identically distributed random variables each with entropy $H(X)$ can be compressed into more than $NH(X)$ bits with negligible risk of information loss, as $N \rightarrow \infty$; but conversely, if they are compressed into fewer than $NH(X)$ bits it is virtually certain that information will be lost.
\end{quotation}

This means that the average bitrate, the compression rate given by the quotient of input bits and output bits of the compression algorithm, which is achievable using lossless compression algorithms is given by the entropy of the data that is to be compressed.
In contrast, lossy compression methods can achieve much higher compression rates by allowing the introduction of distortion. Furthermore, they can also adapt their rate of compression based on either the desired amount of distortion or the target compression rate. They can also be combined with a lossless compression method to further optimise their compression rate. 
Another important factor that determines the trade-off between lossless and lossy compression methods is that many applications do not require the perfect reconstruction given by lossless methods. For example, García-Vílchez et al. \citep{garcia-vilchez_impact_2011} showed that in hyperspectral image classification lossy compression of the image does not always reduce the performance of the classifier. For some lossy compression algorithms the classifier even produced better results on the compressed data than the uncompressed data. This is explained by an introduction of smoothing from the compression which is advantageous for the \ac{svm} classifier used in the experiment. 
For these reasons this thesis focuses on lossy compression methods.

Lossy compression algorithms can be further categorized into traditional compression methods and learning-based compression methods. Traditional compression methods employ algorithms such as the discrete cosine transform which is used in the \ac{jpeg} image compression standard or the wavelet transform, often in combination with a lossless entropy coding method. Examples of traditional methods other than \ac{jpeg} are \ac{btc} which splits images into blocks and then rounds pixel values inside the blocks efficiently and the \ac{heic} standard, which uses wavelet transforms similar to \ac{jpeg} \citep{delp_image_1979,hannuksela_high_2015}. Learning-based approaches use the powerful capability of artificial neural networks to universally approximate arbitrary functions using gradient descent \citep{ruder_overview_2017}. These networks are then used to build models that learn to compress and decompress images. Lately these networks have been shown to outperform traditional compression methods for many applications, especially for \ac{rgb} images using for example the hyperprior model given by Ballé et al. which will be discussed in detail in \autoref{sec:ch2hyperspectral} \citep{balle_end--end_2017,balle_variational_2018,minnen_joint_2018}. 
Research into compression for hyperspectral images is in its early stages, however even in this domain there are promising results showing the capabilities of learning-based autoencoders for this task \citep{kuester_1d-convolutional_2021,kuester_transferability_2022,la_grassa_hyperspectral_2022,guo_learned_2021}.

\section{Objective\label{sec:objective}}
This thesis addresses the problem of hyperspectral image compression using machine learning models consisting of two parts, an encoder and a decoder.
The encoder learns to map the original image to a low-dimensional latent space, thereby performing compression. Analogously, the decoder is trained to reconstruct the input by mapping elements from this latent dimension to full-size images that are as close as possible to the original image.

Contributions to the research on learning-based hyperspectral image compression are made in this thesis by introducing a new architecture that enables the use of spatial compression algorithms such as models that might be used for \ac{rgb} image compression by combining them with a model that performs compression in the spectral domain while keeping spatial relationships intact. In this way, it is possible to compress the spatial information in hyperspectral images using models that are not ordinarily applicable to such images.

Using this architecture, multiple model combinations are designed and compared with each other as well as with the base versions of these models.
These models include models based on \acp{cnn} as well as transformer-based architectures.
A model using a hyperprior architecture is also used for the spatial compression. This architecture yields much higher compression ratios than other models by using an arithmetic encoder in the bottleneck of the compression model. In this way compression rates much higher than the current state of the art for hyperspectral image compression are achieved while distortion is only reduced by a comparatively small amount. 

Additionally, the latent spaces of both the encoder in the spectral domain as well as the latent space of the spatial encoder will be analysed.

\section{Outline\label{sec:outline}}
This section gives a brief introduction of the 7 chapters into which this thesis is split.

\textbf{\autoref{cha:chapter2}} presents the related work. This relates to the research of learning-based hyperspectral image compression by itself. However, as learning-based compression for hyperspectral images, in contrast to traditional hyperspectral compression methods, is a recent and relatively unexplored field of study, the adjacent research topic of learning-based \ac{rgb} image compression is also explored. There is also some exploration of the general studies done on convolutional neural networks as well as transformers.

\textbf{\autoref{cha:chapter3}} gives an overview of the theoretical ideas used in the proposed methods, such as \acp{cnn}, transformers, arithmetic coding as well as the hyperprior architecture for compression.

\textbf{\autoref{cha:chapter4}} details the individual submodels making up the models that are proposed to address the hyperspectral image compression problem as well as the models in their totality. It also gives an overview over the loss functions used for training the models, one of which is a loss function specifically designed for the models proposed in this thesis.

\textbf{\autoref{cha:chapter5}} explains the design of the performed experiments, the dataset that is used for these experiments as well as the results of these experiments. It also details some of the challenges encountered during the experimentation phase.

\textbf{\autoref{cha:chapter6}} gives a summary of the results from the thesis as well as possible improvements that could be made as well as ideas that could be explored in future research.